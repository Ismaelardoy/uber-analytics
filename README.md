# ğŸš– Uber Analytics Pipeline

[![Python](https://img.shields.io/badge/Python-3.12-blue)](https://www.python.org/)
[![Spark](https://img.shields.io/badge/Apache%20Spark-3.5-orange)](https://spark.apache.org/)
[![Terraform](https://img.shields.io/badge/Terraform-1.13.5-623CE4)](https://www.terraform.io/)
[![GCP](https://img.shields.io/badge/GCP-Google%20Cloud-green)](https://cloud.google.com/)
[![License](https://img.shields.io/badge/License-MIT-lightgrey)](LICENSE)

## ğŸ”¹ Project Overview

A **modular ETL pipeline** for processing Uber trip datasets, from raw ingestion to cloud storage, ready for analytics and machine learning.

### Data Flow


```mermaid
flowchart TD
    A[Zones Ingestion] --> C[Transformation]
    B[TripData Ingestion] --> C[Transformation]
    C --> D[Export to Local]
    C --> E[Export to GCP]
```

## ğŸ›  Technologies & Tools

- **Languages:** Python, SQL, Bash  
- **Big Data & ETL:** Apache Spark, PySpark, Data Lakes  
- **Cloud:** Google Cloud Platform (GCP)  
- **Infrastructure:** Terraform for automated bucket provisioning  
- **Version Control:** Git/GitHub  
- **Others:** Docker, Mage AI  

---

## ğŸ“¦ Pipeline Overview

This project implements a **modular ETL pipeline** to process Uber trip datasets from raw ingestion to cloud storage, ready for analytics or machine learning tasks.

1. **Data Ingestion**  
   - Download and read large datasets from public Uber sources.  
   - Supports Parquet formats.  

2. **Data Transformation**  
   - Clean and normalize data.  
   - Calculate derived metrics like trip duration, pickup hour, and weekday.  
   - Filter relevant trips and join with geographic zone info.  

3. **Load / Export**  
   - Store the processed data in a **Google Cloud Storage bucket** using Mage AI.  
   - Fully automated, reproducible pipeline.

---

## âš¡ Key Features

- Modular architecture using **Mage AI** blocks.  
- Cloud infrastructure managed with **Terraform**.  
- Optimized transformations with **PySpark** for large-scale data.  
- ETL best practices: error handling, logging, and reproducible workflow.

---

## ğŸ“ Project Structure

uber-analytics/<br>
â”œâ”€ .file_versions/                   # Internal file versions (local version control)<br>
â”œâ”€ .ssh_tunnel/                      # SSH tunnel configurations and scripts<br>
â”œâ”€ data/                             # Main data<br>
â”œâ”€ data_exporters/                   # Mage blocks to export data to other formats/systems<br>
â”œâ”€ data_loaders/                     # Mage blocks to load data<br>
â”œâ”€ pipelines/<br>
â”‚  â””â”€ uber_etl/                      # Pipeline folder for Uber-specific ETL processes in Mage<br>
â”œâ”€ src/                              # Source code<br>
â”‚  â”œâ”€ etl/                           # ETL functions (ingest, transform, load)<br>
â”‚  â””â”€ analysis/                      # Data analysis scripts and notebooks<br>
â”œâ”€ terraform/                        # Terraform scripts (e.g., GCP bucket creation)<br>
â”œâ”€ transformers/                     # Mage block to transform data<br>
â”œâ”€ .gitignore                         # Files and folders ignored by Git<br>
â”œâ”€ Pipfile                            # Dependencies managed with Pipenv<br>
â”œâ”€ Pipfile.lock                       # Pipenv lockfile<br>
â”œâ”€ README.md                           # Main project documentation<br>
â”œâ”€ metadata.yaml                       # Project metadata<br>


---

## ğŸš€ How to Run

1. Set up your GCP credentials:
```bash
export GOOGLE_APPLICATION_CREDENTIALS="$(pwd)/terraform/keys/my-key.json"
```
2. Initialize Terraform and create the bucket:
```bash
cd terraform
terraform init
terraform apply
```
3. Start the Mage AI pipeline:
```bash
mage start
```


